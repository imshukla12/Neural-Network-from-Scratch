# Neural-Network-from-Scratch

In this project, I designed and implemented a neural network from scratch using only Numpy, along with some helpful preprocessing and cross-validation techniques, to tackle a challenging classification problem with a given dataset.

Key Features:

* Architecture Variations: To explore the impact of different network architectures, I experimented with both single and double hidden layers. This allowed me to compare their performance and understand the complexity required for solving the classification task effectively.
  
* Activation Functions: Activation functions play a crucial role in shaping the neural network's output. I leveraged three popular activation functions - tanh, softmax, and relu. Each activation function brings its unique properties to the model, enabling it to capture non-linearity and improve overall accuracy.
  
* Hyperparameter Tuning: Achieving optimal performance relies on fine-tuning hyperparameters. I diligently adjusted hyperparameters like learning rate, batch size, and the number of hidden units.
